{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import numpy as np\n",
    "import datasets\n",
    "import os\n",
    "import io\n",
    "import ast\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable the specific UserWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using the model-agnostic default `max_length`\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model.to(mps_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique elements in different columns for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_task_unique_class = ['figure question answering','visual question answering','math word problem','geometry problem solving','textbook question answering']\n",
    "metadata_category_unique_class = ['general-vqa','math-targeted-vqa']\n",
    "answer_type_unique_class = ['integer','text','float','list']\n",
    "metadata_context_unique_class = ['scatter plot','synthetic scene','table','geometry diagram','bar chart','abstract scene','function plot','line plot','natural image','puzzle test','scientific figure','pie chart','map chart','medical image','document image','radar chart','heatmap chart','word cloud']\n",
    "metadata_grade_unique_class = ['daily life','elementary school','high school','college']\n",
    "metadata_language_unique_class = ['chinese','english', 'persian']\n",
    "\n",
    "#'metadata_category', 'metadata_task', 'metadata_context', 'metadata_grade', 'metadata_language'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blip Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BLIP_Generate_Caption(image_data):\n",
    "\n",
    "    raw_image = image_data.convert('RGB')\n",
    "    raw_image_tensor = torch.tensor(np.array(raw_image)).permute(2, 0, 1).unsqueeze(0).to(mps_device)\n",
    "\n",
    "    # conditional image captioning\n",
    "    text = \"a photography of\"\n",
    "    inputs = processor(raw_image_tensor, text, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(mps_device) for key, tensor in inputs.items()}\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "\n",
    "    # unconditional image captioning\n",
    "    inputs = processor(raw_image_tensor, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(mps_device) for key, tensor in inputs.items()}\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    return (processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# Assuming 'decoded_image' contains the PIL image object\n",
    "# print(BLIP_Generate_Caption(dataset[\"testmini\"][12]['decoded_image']))\n",
    "\n",
    "def BLIP_Generate_Caption_from_URL(image_url):\n",
    "    # Fetch image from URL\n",
    "    response = requests.get(image_url)\n",
    "    image_data = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Convert image data to tensor\n",
    "    raw_image = image_data.convert('RGB')\n",
    "    raw_image_tensor = torch.tensor(np.array(raw_image)).permute(2, 0, 1).unsqueeze(0).to(mps_device)\n",
    "\n",
    "    # conditional image captioning\n",
    "    text = \"a photography of\"\n",
    "    inputs = processor(raw_image_tensor, text, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(mps_device) for key, tensor in inputs.items()}\n",
    "    out = model.generate(**inputs)\n",
    "\n",
    "    # unconditional image captioning\n",
    "    inputs = processor(raw_image_tensor, return_tensors=\"pt\")\n",
    "    inputs = {key: tensor.to(mps_device) for key, tensor in inputs.items()}\n",
    "    out = model.generate(**inputs)\n",
    "\n",
    "    return (processor.decode(out[0], skip_special_tokens=True))\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()                                             # Make Lower case\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                             # Removes non alphanumeric and spaces\n",
    "    tokens = word_tokenize(text)                                    # Tokenizing words\n",
    "    stop_words = set(stopwords.words('english'))                    # Obtaining stop words like 'a', 'the' etc.\n",
    "    tokens = [word for word in tokens if word not in stop_words]    # Removing stop words\n",
    "    lemmatizer = WordNetLemmatizer()                                # Obtaining base of words like 'cat' and not 'cats'\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]        # lemmatizing each word. \n",
    "    preprocessed_text = ' '.join(tokens)                            # Joining and returning\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_mode():\n",
    "    print (\"Enter 1 if only the question is passed                  : \")\n",
    "    print (\"Enter 2 if only the image URL is passed                 : \")\n",
    "    print (\"Enter 3 if both Question and Image URL is to be passed  : \")\n",
    "    print (\"Enter 4 to exit program                                 : \")\n",
    "    choice = int(input(\"Enter choice :\"))\n",
    "    if (choice not in [1,2,3]):\n",
    "        print (\"Enter a valid choice again: \")\n",
    "        prediction_mode()\n",
    "    else :\n",
    "        return choice\n",
    "\n",
    "    # print( prediction_mode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to choose Data Frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_splitter_and_vectorizer(df):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], \n",
    "                                                    df[['answer_type','metadata_language', 'metadata_skills', 'metadata_task', \n",
    "                                                        'metadata_category', 'metadata_context', 'metadata_grade',\n",
    "                                                        'metadata_split', 'metadata_source', 'metadata_img_height', \n",
    "                                                        'metadata_img_width']], \n",
    "                                                    test_size=0.4, random_state=42)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Fit the vectorizer on the preprocessed text data and transform it\n",
    "            X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "            return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for only image promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_text_processor_for_onlyImage(text, answer_type, X_train_tfidf, y_train):\n",
    "    print (text)\n",
    "    # Instantiate SVM classifier\n",
    "    SVM = svm.SVC(C=10, kernel='linear', degree=2, gamma='auto')\n",
    "    \n",
    "    # Fit the SVM classifier on the training dataset\n",
    "    SVM.fit(X_train_tfidf, y_train[answer_type])\n",
    "\n",
    "    # Preprocess the input text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    #print(\"Preprocessed text:\", preprocessed_text)\n",
    "\n",
    "    # Transform preprocessed text using tfidf_vectorizer\n",
    "    preprocessed_text = tfidf_vectorizer.transform([preprocessed_text])\n",
    "\n",
    "    # Predict the label\n",
    "    prediction = SVM.predict(preprocessed_text)\n",
    "\n",
    "    # Print the prediction\n",
    "    print(\"Prediction for \"+answer_type+\" :\", prediction)\n",
    "\n",
    "\n",
    "            \n",
    "def predictor_for_URL( URL, X_train_tfidf, y_train):\n",
    "    print (URL)\n",
    "    response = requests.get(URL)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.show()\n",
    "    for answer_type in ['answer_type','metadata_category', 'metadata_task', 'metadata_context', 'metadata_grade', 'metadata_language']:\n",
    "        test_text_processor_for_onlyImage(BLIP_Generate_Caption_from_URL(URL), answer_type, X_train_tfidf, y_train)\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for only Question Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_for_Question( Question, X_train_tfidf, y_train):\n",
    "\n",
    "    for answer_type in ['answer_type','metadata_category', 'metadata_task', 'metadata_context', 'metadata_grade', 'metadata_language']:\n",
    "        test_text_processor_for_onlyImage(Question, answer_type, X_train_tfidf, y_train)\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function For Both Image And Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_for_Question_and_URL( Question, URL, X_train_tfidf, y_train):\n",
    "    print (URL)\n",
    "    response = requests.get(URL)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.show()\n",
    "    for answer_type in ['answer_type','metadata_category', 'metadata_task', 'metadata_context', 'metadata_grade', 'metadata_language']:\n",
    "        test_text_processor_for_onlyImage(BLIP_Generate_Caption_from_URL(URL)+\" \"+Question, answer_type, X_train_tfidf, y_train)\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_paths():\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "        This Module will get the path of the 3 DATA FRAMES. \n",
    "        It will return csv_path which is a list whose each element is path to the dataframe\n",
    "        \n",
    "    \"\"\"\n",
    "     \n",
    "    # Get the current directory\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # Navigate one folder back to the parent directory\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "    \n",
    "\n",
    "    # Navigate to the \"data\" folder inside the parent directory\n",
    "    data_directory = os.path.join(parent_directory, \"Data\")\n",
    "\n",
    "    # Navigate to the \"processed\" subfolder inside the \"data\" folder\n",
    "    processed_directory = os.path.join(data_directory, \"PROCESSED\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Define the paths to the data files\n",
    "    csv_file_paths = [\n",
    "        os.path.join(processed_directory, \"df_for_only_question.csv\"),\n",
    "        os.path.join(processed_directory, \"df_for_image_captions.csv\"),\n",
    "        os.path.join(processed_directory, \"df_for_image_captions+Questions.csv\"),\n",
    "    ]\n",
    "\n",
    "    return csv_file_paths\n",
    "\n",
    "csv_file_paths = get_df_paths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/psinha/Desktop/Structred Folder/TEST\n"
     ]
    }
   ],
   "source": [
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predictor():\n",
    "    \n",
    "    choice = prediction_mode()\n",
    "\n",
    "    if (choice == 1):\n",
    "        df = pd.read_csv(csv_file_paths[0])\n",
    "        X_train_tfidf, X_test_tfidf, y_train, y_test = df_splitter_and_vectorizer(df)\n",
    "        Question = input(\"Enter Quesion\")       \n",
    "        print (\"\\n\")\n",
    "        predictor_for_Question( Question, X_train_tfidf, y_train)\n",
    "\n",
    "    elif (choice == 2):\n",
    "        df = pd.read_csv(csv_file_paths[1])\n",
    "        X_train_tfidf, X_test_tfidf, y_train, y_test = df_splitter_and_vectorizer(df)\n",
    "        print('Enter Image URL (press Enter to skip): ')\n",
    "        URL = input().strip('\"')\n",
    "        print (\"\\n\")\n",
    "        predictor_for_URL( URL, X_train_tfidf, y_train)   \n",
    "        \n",
    "    elif (choice == 3):\n",
    "        df = pd.read_csv(csv_file_paths[2])\n",
    "        X_train_tfidf, X_test_tfidf, y_train, y_test = df_splitter_and_vectorizer(df)\n",
    "        #print('Enter Image URL (press Enter to skip): ')\n",
    "        Question = input(\"Enter Quesion\")\n",
    "        URL = input('Enter Image URL (press Enter to skip): ').strip('\"')\n",
    "        print (\"\\n\")\n",
    "        predictor_for_Question_and_URL( Question, URL, X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "    else:\n",
    "        exit()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 if only the question is passed                  : \n",
      "Enter 2 if only the image URL is passed                 : \n",
      "Enter 3 if both Question and Image URL is to be passed  : \n",
      "Enter 4 to exit program                                 : \n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for answer_type : ['float']\n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for metadata_category : ['general-vqa']\n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for metadata_task : ['figure question answering']\n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for metadata_context : ['bar chart']\n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for metadata_grade : ['daily life']\n",
      "\n",
      "\n",
      "What is India?\n",
      "Prediction for metadata_language : ['english']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_predictor()\n",
    "\n",
    "# QUESTION : WHAT IS THE SURFACE INTEGRAL OF THE FOLLOWING FIGURE \n",
    "# QUESTION : WHAT IS THE AREA OF THE FIGURE SHOWN BELOW. \n",
    "\n",
    "\n",
    "# HORSE PULLING A CAR : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSIkJDpcdIMFHRjsgvc2JD0RspkUStSXuoIBQ&usqp=CAU\n",
    "# CAT GIVING HIGH FIVE TO STAUTE OF LIBERTY : https://randomwordgenerator.com/img/picture-generator/52e1d5424b56aa14f1dc8460962e33791c3ad6e04e50744074267bd69149c7_640.jpg\n",
    "# DIES IMAGE : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTzqqxGuOfwNRD61afNt4iX0eBmvcZfCWx5Tg&usqp=CAU\n",
    "# FOOD CHAIN : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSO-8eWbdZ5aEbz1PJ1ryIFYuuG18u8tCc0YXxg33av7SatC5Er8TK9PplEa6IdXsTbQFQ&usqp=CAU\n",
    "\n",
    "\n",
    "# MAP QUESTION : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSY3NVSNmEeGeEcwByTywefdCi2fncYFkgPcA&usqp=CAU\n",
    "# PUZZLE PROBLEM : https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRpcIq0dBVcbISQ9c9lIcqFtd7OnUy9U5reFg&usqp=CAU  # Slight miss classification because of word cones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_task_unique_class = ['figure question answering','visual question answering','math word problem','geometry problem solving','textbook question answering']\n",
    "# metadata_category_unique_class = ['general-vqa','math-targeted-vqa']\n",
    "# answer_type_unique_class = ['integer','text','float','list']\n",
    "# metadata_context_unique_class = ['scatter plot','synthetic scene','table','geometry diagram','bar chart','abstract scene','function plot','line plot','natural image','puzzle test','scientific figure','pie chart','map chart','medical image','document image','radar chart','heatmap chart','word cloud']\n",
    "# metadata_grade_unique_class = ['daily life','elementary school','high school','college']\n",
    "# metadata_language_unique_class = ['english','chinese']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation_of_model(class_name, class_name_unique_class):\n",
    "\n",
    "    print(\"Enter 1 if only question given              :\")\n",
    "    print(\"Enter 2 if only image given                 :\")\n",
    "    print(\"Enter 3 if both question and image given    :\")\n",
    "    choice = int(input(\"ENTER YOUR INPUT TYPE          :\"))\n",
    "\n",
    "    if choice == 1:\n",
    "        df = pd.read_csv('df_for_only_question.csv')\n",
    "\n",
    "    elif choice == 2:\n",
    "        df = pd.read_csv('df_for_image_captions.csv')\n",
    "\n",
    "    elif choice == 3:\n",
    "        df = pd.read_csv('df_for_image_captions+Questions.csv')\n",
    "\n",
    "    else:\n",
    "        print(\"INVALID CHOICE \")\n",
    "\n",
    "    # Assuming df_splitter_and_vectorizer and other necessary functions are defined elsewhere\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test = df_splitter_and_vectorizer(df)\n",
    "\n",
    "    # Fit the training dataset on the SVM classifier\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train_tfidf, y_train[class_name])\n",
    "\n",
    "    # Predict the labels on the validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test_tfidf)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test[class_name], predictions_SVM)\n",
    "\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=class_name_unique_class)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cm_display.plot(ax=ax, xticks_rotation='vertical')  # Rotating x-axis labels vertically\n",
    "    ax.set_title(\"Confusion Matrix For \"+class_name)  # Setting the title\n",
    "\n",
    "    # Print classification report\n",
    "    report = classification_report(y_test[class_name], predictions_SVM, target_names=class_name_unique_class)\n",
    "    print(\"\\nClassification Report For \"+class_name+\"\\n\", report)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# evaluation_of_model(\"metadata_task\", metadata_task_unique_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_of_model (\"metadata_task\", metadata_task_unique_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_of_model (\"metadata_language\", metadata_language_unique_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
