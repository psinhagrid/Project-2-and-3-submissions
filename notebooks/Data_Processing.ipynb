{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "src_data_directory = os.path.join(parent_directory, \"src\", \"data\")\n",
    "\n",
    "\n",
    "sys.path.append(src_data_directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_cleaner' from '/Users/psinha/Desktop/Structred Folder/src/data/data_cleaner.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_cleaner\n",
    "importlib.reload(data_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load our data from hugging face library\n",
    "\n",
    "train_data = load_dataset(\"AI4Math/MathVista\", split=\"test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Mac GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Blip model fro transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processor and caption generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BLIP_Caption_Preprocessor_And_Generator(image_data):\n",
    "\n",
    "    raw_image = image_data.convert('RGB')                                                               # Convert to RGB format\n",
    "    raw_image_tensor = torch.tensor(np.array(raw_image)).permute(2, 0, 1).unsqueeze(0).to(mps_device)   # make into torch tensor to push into mps device\n",
    "\n",
    "\n",
    "\n",
    "    inputs = processor(raw_image_tensor, return_tensors=\"pt\")                                           # processor does some additional processing for inputs specefic to model                                 \n",
    "    inputs = {key: tensor.to(mps_device) for key, tensor in inputs.items()}                             # Transfer inputs to mps_device\n",
    "\n",
    "    out = model.generate(**inputs)                                                                      # Generate captions\n",
    "    return (processor.decode(out[0], skip_special_tokens=True))                                         # Return decoded values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-english rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the function made by us to remove non english rows\n",
    "\n",
    "\n",
    "# Apply language detection to each text entry in the DataFrame\n",
    "df['language'] = df['questions'].apply(data_cleaner.detect_language)\n",
    "\n",
    "# Filter out non-English entries\n",
    "df = df[df['language'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "# Drop the language column as it's no longer needed\n",
    "df.drop(columns=['language'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make processed text as a columns in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'preprocessed_text' column\n",
    "df['preprocessed_text'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can save captions from our dataset into data frames in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_trainer(df, chunk_size, chunk_number, train_data):\n",
    "\n",
    "    \"\"\"\n",
    "        This is used ot train data in smaller chunks\n",
    "        The return type will be dataframe\n",
    "    \"\"\"\n",
    "    start_index = chunk_number * chunk_size                                                 # Starting address of Chunk.\n",
    "    end_index = min((chunk_number + 1) * chunk_size, len(train_data))                       # If number end of data frame comes before chunk end.\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        example = train_data[i]\n",
    "        text = example['question']                                                          # Obtaining the question \n",
    "        caption = BLIP_Caption_Preprocessor_And_Generator(example['decoded_image'])\n",
    "        caption += text                                                                     # Combining Captions and text\n",
    "        preprocessed_text = data_cleaner.preprocess_text(caption)                           # Adding to our processed text columns\n",
    "\n",
    "        # Update the preprocessed_text column in the DataFrame\n",
    "        df.at[i, 'preprocessed_text'] = preprocessed_text                                   # Adding to the corresponding row number\n",
    "\n",
    "        print (\"Processed Image : \",i)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Processed chunk {chunk_number}\")\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 0, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 1, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 3, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 4, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 5, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 6, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 7, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 8, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = chunk_trainer(df, 500, 9, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_name.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
